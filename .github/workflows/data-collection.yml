name: Howard County Data Collection Pipeline

on:
  schedule:
    # Run every 4 hours during business days
    - cron: '0 */4 * * 1-5'
    # Run once daily on weekends
    - cron: '0 8 * * 0,6'
  workflow_dispatch: # Allow manual triggers
  push:
    paths:
      - 'scrapers/**'
      - '.github/workflows/data-collection.yml'

env:
  PYTHON_VERSION: '3.11'

jobs:
  scrape-government-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Python Dependencies
      run: |
        pip install -r requirements.txt

    - name: Create Output Directories
      run: |
        mkdir -p data/county-council data/emergency data/videos data/logs logs

    - name: Run County Council Scraper
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        python scrapers/county-council-scraper.py
        
    - name: Run Emergency Alert Monitor
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        TWILIO_ACCOUNT_SID: ${{ secrets.TWILIO_ACCOUNT_SID }}
        TWILIO_AUTH_TOKEN: ${{ secrets.TWILIO_AUTH_TOKEN }}
        SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}
      run: |
        python scrapers/emergency-alert-monitor.py
        
    - name: Run Video Discovery
      env:
        YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python scrapers/video-discovery.py

    - name: Validate Scraped Data
      run: |
        if [ -f "scripts/validate-data.py" ]; then
          python scripts/validate-data.py
        else
          echo "Validation script not found, skipping validation..."
        fi

    - name: Upload Artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraped-data-${{ github.run_id }}
        path: |
          data/**/*.json
          data/**/*.csv
          logs/**/*.log
        retention-days: 30

    - name: Trigger n8n Workflow
      if: success()
      run: |
        if [ -n "${{ secrets.N8N_WEBHOOK_URL }}" ]; then
          curl -X POST "${{ secrets.N8N_WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "source": "government-data",
              "run_id": "${{ github.run_id }}",
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
              "status": "completed"
            }' || echo "Failed to trigger n8n webhook"
        else
          echo "N8N_WEBHOOK_URL not configured, skipping webhook trigger"
        fi

  notify-completion:
    runs-on: ubuntu-latest
    needs: scrape-government-data
    if: always()
    
    steps:
    - name: Notify Job Status
      run: |
        if [ "${{ needs.scrape-government-data.result }}" = "success" ]; then
          echo "✅ Data collection completed successfully"
        else
          echo "❌ Data collection failed"
        fi